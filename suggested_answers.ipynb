{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "##  Python Data Science\n",
    "\n",
    "> Exercises: Introduction to PySpark\n",
    "\n",
    "Kuo, Yao-Jen <yaojenkuo@datainpoint.com> from [DATAINPOINT](https://www.datainpoint.com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import concat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a function called `create_spark_session` that is able to return 2 objects: a SparkContext and a SparkSession.\n",
    "\n",
    "- Expected inputs: None.\n",
    "- Expected outputs: a SparkContext and a SparkSession."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spark_session():\n",
    "    \"\"\"\n",
    "    >>> sc, spark = create_spark_session()\n",
    "    >>> print(type(sc))\n",
    "    <class 'pyspark.context.SparkContext'>\n",
    "    >>> print(type(spark))\n",
    "    <class 'pyspark.sql.session.SparkSession'>\n",
    "    >>> print(sc.version)\n",
    "    3.0.0\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    sc = SparkContext.getOrCreate()\n",
    "    spark = SparkSession(sc)\n",
    "    return sc, spark\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a function named `create_spark_dataframe` that is able to create a Spark DataFrame from scratch.\n",
    "\n",
    "- Expected inputs: a SparkSession.\n",
    "- Expected outputs: a (4, 3) Spark DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spark_dataframe(spark):\n",
    "    \"\"\"\n",
    "    >>> sc, spark = create_spark_session()\n",
    "    >>> spark_dataframe = create_spark_dataframe(spark)\n",
    "    >>> print(type(spark_dataframe))\n",
    "    <class 'pyspark.sql.dataframe.DataFrame'>\n",
    "    >>> spark_dataframe.show()\n",
    "    +--------------------+------------+-----------+\n",
    "    |               title|release_year|imdb_rating|\n",
    "    +--------------------+------------+-----------+\n",
    "    |        The Avengers|        2012|        8.0|\n",
    "    |Avengers: Age of ...|        2015|        7.3|\n",
    "    |Avengers: Infinit...|        2018|        8.4|\n",
    "    |   Avengers: Endgame|        2019|        8.4|\n",
    "    +--------------------+------------+-----------+\n",
    "    >>> print(spark_dataframe.count())\n",
    "    4\n",
    "    >>> print(len(spark_dataframe.dtypes))\n",
    "    3\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    columns = [\"title\", \"release_year\", \"imdb_rating\"]\n",
    "    rows = [\n",
    "        (\"The Avengers\", 2012, 8.0),\n",
    "        (\"Avengers: Age of Ultron\", 2015, 7.3),\n",
    "        (\"Avengers: Infinity War\", 2018, 8.4),\n",
    "        (\"Avengers: Endgame\", 2019, 8.4)\n",
    "    ]\n",
    "    spark_df = spark.createDataFrame(rows).toDF(*columns)\n",
    "    return spark_df\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a function named `read_csv_as_spark_dataframe` that is able to create a Spark DataFrame via a external CSV file.\n",
    "\n",
    "- Expected inputs: a SparkSession and a CSV file.\n",
    "- Expected outputs: a (51678, 7) Spark DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_as_spark_dataframe(spark, csv_file):\n",
    "    \"\"\"\n",
    "    >>> sc, spark = create_spark_session()\n",
    "    >>> spark_dataframe = read_csv_as_spark_dataframe(spark, 'presidential.csv')\n",
    "    >>> spark_dataframe.show(5)\n",
    "    +------+------+-------+------+------+-----------+-----+\n",
    "    |county|  town|village|office|number|  candidate|votes|\n",
    "    +------+------+-------+------+------+-----------+-----+\n",
    "    |宜蘭縣|宜蘭市| 民族里|     1|     1|宋楚瑜/余湘|   37|\n",
    "    |宜蘭縣|宜蘭市| 民族里|     2|     1|宋楚瑜/余湘|   31|\n",
    "    |宜蘭縣|宜蘭市| 建軍里|     3|     1|宋楚瑜/余湘|   19|\n",
    "    |宜蘭縣|宜蘭市| 建軍里|     4|     1|宋楚瑜/余湘|   29|\n",
    "    |宜蘭縣|宜蘭市| 泰山里|     5|     1|宋楚瑜/余湘|   25|\n",
    "    +------+------+-------+------+------+-----------+-----+\n",
    "    only showing top 5 rows\n",
    "    >>> print(spark_dataframe.count())\n",
    "    51678\n",
    "    >>> print(len(spark_dataframe.dtypes))\n",
    "    7\n",
    "    >>> spark_dataframe.dtypes[0][1]\n",
    "    'string'\n",
    "    >>> spark_dataframe.dtypes[-1][1]\n",
    "    'int'\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    spark_df = spark.read.csv(csv_file, header=True, inferSchema=True)\n",
    "    return spark_df\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a function named `get_votes_summary` that is able to summarize the total votes by candidate.\n",
    "\n",
    "- Expected inputs: a SparkSession and a CSV file.\n",
    "- Expected outputs: a (3, 3) Spark DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_votes_summary(spark, csv_file):\n",
    "    \"\"\"\n",
    "    >>> sc, spark = create_spark_session()\n",
    "    >>> votes_summary = get_votes_summary(spark, 'presidential.csv')\n",
    "    >>> votes_summary.show()\n",
    "    +------+-------------+----------+\n",
    "    |number|    candidate|sum(votes)|\n",
    "    +------+-------------+----------+\n",
    "    |     1|  宋楚瑜/余湘|    608590|\n",
    "    |     2|韓國瑜/張善政|   5522119|\n",
    "    |     3|蔡英文/賴清德|   8170231|\n",
    "    +------+-------------+----------+\n",
    "    >>> print(votes_summary.count())\n",
    "    3\n",
    "    >>> print(len(votes_summary.dtypes))\n",
    "    3\n",
    "    >>> print(votes_summary.head(3)[0][-1])\n",
    "    608590\n",
    "    >>> print(votes_summary.head(3)[1][-1])\n",
    "    5522119\n",
    "    >>> print(votes_summary.head(3)[2][-1])\n",
    "    8170231\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    spark_df = spark.read.csv(csv_file, header=True, inferSchema=True)\n",
    "    summary_df = spark_df.groupBy('number', 'candidate').sum('votes').sort('number')\n",
    "    return summary_df\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a function named `get_combined_key` that is able to concat 3 columns: county, town, and village into 1 combined key column and let us know many distinct electoral districts are there in Taiwan.\n",
    "\n",
    "- Expected inputs: a SparkSession and a CSV file.\n",
    "- Expected outputs: a (7737, 1) Spark DataFrame.\n",
    "\n",
    "PS Use `concat` function that we've imported for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_combined_key(spark, csv_file):\n",
    "    \"\"\"\n",
    "    >>> sc, spark = create_spark_session()\n",
    "    >>> combined_key = get_combined_key(spark, 'presidential.csv')\n",
    "    >>> combined_key.show(5)\n",
    "    +------------------+\n",
    "    |      combined_key|\n",
    "    +------------------+\n",
    "    |南投縣中寮鄉中寮村|\n",
    "    |南投縣中寮鄉內城村|\n",
    "    |南投縣中寮鄉八仙村|\n",
    "    |南投縣中寮鄉和興村|\n",
    "    |南投縣中寮鄉崁頂村|\n",
    "    +------------------+\n",
    "    only showing top 5 rows\n",
    "    >>> print(combined_key.count())\n",
    "    7737\n",
    "    >>> print(len(combined_key.dtypes))\n",
    "    1\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    spark_df = spark.read.csv(csv_file, header=True, inferSchema=True)\n",
    "    combined_key = spark_df.select(concat(spark_df.county, \n",
    "                                          spark_df.town,\n",
    "                                          spark_df.village).alias(\"combined_key\"))\n",
    "    distinct_combined_key = combined_key.distinct().sort('combined_key')\n",
    "    return distinct_combined_key\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run tests!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_create_spark_dataframe (__main__.TestIntroductionToSpark) ... ok\n",
      "test_create_spark_session (__main__.TestIntroductionToSpark) ... ok\n",
      "test_get_combined_key (__main__.TestIntroductionToSpark) ... ok\n",
      "test_get_votes_summary (__main__.TestIntroductionToSpark) ... ok\n",
      "test_read_csv_as_spark_dataframe (__main__.TestIntroductionToSpark) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 5 tests in 35.109s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "class TestIntroductionToSpark(unittest.TestCase):\n",
    "    def test_create_spark_session(self):\n",
    "        sc, spark = create_spark_session()\n",
    "        self.assertIsInstance(sc, pyspark.context.SparkContext)\n",
    "        self.assertIsInstance(spark, pyspark.sql.session.SparkSession)\n",
    "        self.assertIsInstance(sc.version, str)\n",
    "    def test_create_spark_dataframe(self):\n",
    "        sc, spark = create_spark_session()\n",
    "        spark_dataframe = create_spark_dataframe(spark)\n",
    "        self.assertIsInstance(spark_dataframe, pyspark.sql.dataframe.DataFrame)\n",
    "        self.assertEqual(spark_dataframe.count(), 4)\n",
    "        self.assertEqual(len(spark_dataframe.dtypes), 3)\n",
    "    def test_read_csv_as_spark_dataframe(self):\n",
    "        sc, spark = create_spark_session()\n",
    "        spark_dataframe = read_csv_as_spark_dataframe(spark, 'presidential.csv')\n",
    "        self.assertIsInstance(spark_dataframe, pyspark.sql.dataframe.DataFrame)\n",
    "        self.assertEqual(spark_dataframe.count(), 51678)\n",
    "        self.assertEqual(len(spark_dataframe.dtypes), 7)\n",
    "        self.assertEqual(spark_dataframe.dtypes[0][1], 'string')\n",
    "        self.assertEqual(spark_dataframe.dtypes[-1][1], 'int')\n",
    "    def test_get_votes_summary(self):\n",
    "        sc, spark = create_spark_session()\n",
    "        votes_summary = get_votes_summary(spark, 'presidential.csv')\n",
    "        self.assertIsInstance(votes_summary, pyspark.sql.dataframe.DataFrame)\n",
    "        self.assertEqual(votes_summary.count(), 3)\n",
    "        self.assertEqual(len(votes_summary.dtypes), 3)\n",
    "        self.assertEqual(votes_summary.head(3)[0][-1], 608590)\n",
    "        self.assertEqual(votes_summary.head(3)[1][-1], 5522119)\n",
    "        self.assertEqual(votes_summary.head(3)[2][-1], 8170231)\n",
    "    def test_get_combined_key(self):\n",
    "        sc, spark = create_spark_session()\n",
    "        combined_key = get_combined_key(spark, 'presidential.csv')\n",
    "        self.assertIsInstance(combined_key, pyspark.sql.dataframe.DataFrame)\n",
    "        self.assertEqual(combined_key.count(), 7737)\n",
    "        self.assertEqual(len(combined_key.dtypes), 1)\n",
    "\n",
    "suite = unittest.TestLoader().loadTestsFromTestCase(TestIntroductionToSpark)\n",
    "runner = unittest.TextTestRunner(verbosity=2)\n",
    "test_results = runner.run(suite)\n",
    "number_of_failures = len(test_results.failures)\n",
    "number_of_errors = len(test_results.errors)\n",
    "number_of_test_runs = test_results.testsRun\n",
    "number_of_successes = number_of_test_runs - (number_of_failures + number_of_errors)\n",
    "total_points = number_of_successes * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You've got 5 successes with 10 points.\n"
     ]
    }
   ],
   "source": [
    "print(\"You've got {} successes with {} points.\".format(number_of_successes, total_points))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
